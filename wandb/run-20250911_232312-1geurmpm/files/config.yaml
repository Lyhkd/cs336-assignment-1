_wandb:
    value:
        cli_version: 0.19.11
        m:
            - "1": step
              "6":
                - 3
              "7": []
            - "1": train/wallclock_s
              "5": 1
              "6":
                - 1
                - 3
              "7": []
        python_version: 3.11.13
        t:
            "1":
                - 1
                - 55
            "2":
                - 1
                - 55
            "3":
                - 7
                - 13
                - 16
                - 23
                - 55
                - 61
            "4": 3.11.13
            "5": 0.19.11
            "8":
                - 5
            "12": 0.19.11
            "13": linux-x86_64
batch_size:
    value: 32
checkpoint_dir:
    value: ./checkpoints
compile_model:
    value: false
config:
    value: config/lr7e3-7e4-bs1.json
context_length:
    value: 256
cosine_annealing_iters:
    value: 8000
d_ff:
    value: 1344
d_model:
    value: 512
data_dtype:
    value: uint16
dataset_name:
    value: TinyStories
device:
    value: cuda
dtype:
    value: float32
eval_data_path:
    value: data/TinyStoriesV2-GPT4-valid.txt
eval_every:
    value: 500
experiment_name:
    value: lr7e3-7e4-bs32
log_every:
    value: 50
log_file:
    value: ./checkpoints/lr7e3-7e4-bs32/training.log
max_grad_norm:
    value: 1
max_iterations:
    value: 1280000
max_learning_rate:
    value: 0.007
min_learning_rate:
    value: 0.0007
num_heads:
    value: 8
num_layers:
    value: 4
resume_from:
    value: null
rope_theta:
    value: 10000
save_every:
    value: 1000
train_data_path:
    value: data/TinyStoriesV2-GPT4-train.txt
vocab_size:
    value: 10000
warmup_iters:
    value: 1000
weight_decay:
    value: 0.01
