{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c41047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27682406400\n"
     ]
    }
   ],
   "source": [
    "FLOP = (2 * 1024 * 1600 * 1600 * 3  # QKVproj = 3*2Ld_m^2\n",
    "+ 2 * 25 * 1024 * 1024 * 64 # QKT (bs,h,T,d_k) @ (bs,h,T,d_k) = 2hL^2d_k\n",
    "+ 2 * 25 * 1024 * 1024 * 64 # Attn @ V (bs,h,T,T) @ (bs,h,T,d_k) = 2hL^2d_k\n",
    "+ 2 * 1024 * 1600 * 1600 # Val @ Wo^T (bs,T,d_m) @ d_m, d_m = 2Ld_m^2\n",
    "# + 2 * 1024 * 1600 * 6400 * 3 # Up L,d_m @ d_m,d_ff\n",
    ") * 48 + 2 * 1024 * 1600 * 50256 + 2 * 1024 * 50256 * 1600 # final proj\n",
    "print(FLOP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41adb6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def tokenize_text_to_binary(\n",
    "    text_path: str,\n",
    "    output_path: str,\n",
    "    vocab_path: str = \"data/TinyStories_train/vocab_str.json\",\n",
    "    merges_path: str = \"data/TinyStories_train/merges_str.json\",\n",
    "    special_tokens: list[str] = (\"<|endoftext|>\",),\n",
    "    dtype: np.dtype = np.int32,\n",
    ") -> int:\n",
    "    from cs336_basics.tokenizer import Tokenizer\n",
    "\n",
    "    os.makedirs(os.path.dirname(output_path) or \".\", exist_ok=True)\n",
    "    tokenizer = Tokenizer.from_files(vocab_path, merges_path, list(special_tokens))\n",
    "    \n",
    "    with open(text_path, \"r\", encoding=\"utf-8\") as fin, open(output_path, \"wb\") as fout:\n",
    "        tokens: Iterable[int] = tokenizer.encode_iterable(fin)\n",
    "        \n",
    "        # 使用tqdm显示进度条\n",
    "        with tqdm(desc=\"标记化进度\", unit=\"tokens\", unit_scale=True) as pbar:\n",
    "            for tok in tokens:\n",
    "                # 将int转换为指定dtype的numpy数组再写入字节\n",
    "                fout.write(np.array([tok], dtype=dtype).tobytes())\n",
    "                pbar.update(1)\n",
    "\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23ed6a04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention:  96636764160 (15.66%) FFN:  362387865600 (58.72%) Projection:  158094852096 (25.62%)\n",
      "617119481856\n",
      "Attention:  309237645312 (20.80%) FFN:  966367641600 (65.01%) Projection:  210793136128 (14.18%)\n",
      "1486398423040\n",
      "Attention:  676457349120 (24.58%) FFN:  1811939328000 (65.84%) Projection:  263491420160 (9.57%)\n",
      "2751888097280\n",
      "Attention:  1328755507200 (28.40%) FFN:  3019898880000 (64.56%) Projection:  329364275200 (7.04%)\n",
      "4678018662400\n",
      "Attention:  98569499443200 (64.78%) FFN:  48318382080000 (31.76%) Projection:  5269828403200 (3.46%)\n",
      "152157709926400\n"
     ]
    }
   ],
   "source": [
    "def cal_FLOP(layer=48, d_model=1600, heads=25, context_length=1024, d_ff=6400, vocab_size=50257):\n",
    "    d_k = d_model // heads\n",
    "    FLOP = (2 * context_length * d_model * d_model * 3  # QKVproj = 3*2Ld_m^2 \n",
    "    + 2 * heads * context_length * context_length * d_k # QKT (bs,h,T,d_k) @ (bs,h,T,d_k) = 2hL^2d_k\n",
    "    + 2 * heads * context_length * context_length * d_k # Attn @ V (bs,h,T,T) @ (bs,h,T,d_k) = 2hL^2d_k\n",
    "    + 2 * context_length * d_model * d_model # Val @ Wo^T (bs,T,d_m) @ d_m, d_m = 2Ld_m^2\n",
    "    + 2 * context_length * d_model * d_ff * 3 ) * layer + 2 * context_length * vocab_size * d_model  + 2 * context_length * d_model * vocab_size # final proj\n",
    "    Attention = layer * (2 * context_length * d_model * d_model * 3 + 2 * heads * context_length * context_length * d_k\n",
    "    + 2 * heads * context_length * context_length * d_k\n",
    "    + 2 * context_length * d_model * d_model\n",
    "    )\n",
    "    FFN = layer * (2 * context_length * d_model * d_ff * 3)\n",
    "    Projection = 2 * context_length * vocab_size * d_model  + 2 * context_length * d_model * vocab_size\n",
    "    print(\"Attention: \", Attention, f\"({Attention / FLOP:.2%})\", \"FFN: \", FFN, f\"({FFN / FLOP:.2%})\", \"Projection: \", Projection, f\"({Projection / FLOP:.2%})\")\n",
    "    return FLOP\n",
    "\n",
    "\n",
    "print(cal_FLOP(layer=12, d_model=768, heads=12))\n",
    "print(cal_FLOP(layer=24, d_model=1024, heads=16))\n",
    "print(cal_FLOP(layer=36, d_model=1280, heads=20))\n",
    "print(cal_FLOP())\n",
    "print(cal_FLOP(context_length=16384))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pi0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
