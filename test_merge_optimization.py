#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""æµ‹è¯•BPEåˆå¹¶ä¼˜åŒ–çš„æ€§èƒ½æå‡"""

import time
import os
from cs336_basics.bpe import train_bpe

def create_test_data():
    """åˆ›å»ºä¸€ä¸ªç¨å¤§çš„æµ‹è¯•æ•°æ®æ–‡ä»¶"""
    test_content = """
    Hello world! This is a test file for BPE training.
    We want to see how the optimized merge algorithm performs compared to the traditional one.
    The quick brown fox jumps over the lazy dog.
    Python is a great programming language for natural language processing.
    BPE (Byte Pair Encoding) is a popular tokenization method used in modern NLP models.
    <|endoftext|>
    Machine learning has revolutionized the field of artificial intelligence.
    Deep learning models like transformers have achieved remarkable success in various tasks.
    Natural language understanding requires sophisticated algorithms and large datasets.
    <|endoftext|>
    The optimization of BPE training involves caching pair frequencies and incremental updates.
    This approach significantly reduces the computational complexity of the merging step.
    By only updating the affected pairs, we can achieve substantial speedups.
    <|endoftext|>
    """ * 10  # é‡å¤10æ¬¡ä»¥å¢åŠ æ•°æ®é‡
    
    with open("test_merge_data.txt", "w", encoding="utf-8") as f:
        f.write(test_content)
    
    return "test_merge_data.txt"

def test_merge_optimization():
    """æµ‹è¯•åˆå¹¶ä¼˜åŒ–çš„æ€§èƒ½æ•ˆæœ"""
    print("=" * 70)
    print("BPEåˆå¹¶ç®—æ³•ä¼˜åŒ–æ€§èƒ½æµ‹è¯•")
    print("=" * 70)
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    test_file = create_test_data()
    file_size = os.path.getsize(test_file)
    print(f"æµ‹è¯•æ–‡ä»¶å¤§å°: {file_size} bytes")
    
    special_tokens = ["<|endoftext|>"]
    vocab_size = 800
    
    print(f"è¯æ±‡è¡¨ç›®æ ‡å¤§å°: {vocab_size}")
    print(f"ç‰¹æ®Šæ ‡è®°: {special_tokens}")
    print("-" * 70)
    
    # æµ‹è¯•ä¼˜åŒ–ç®—æ³•
    print("ğŸš€ æµ‹è¯•ä¼˜åŒ–çš„åˆå¹¶ç®—æ³•...")
    start_time = time.time()
    vocab_opt, merges_opt = train_bpe(
        input_path=test_file,
        vocab_size=vocab_size,
        special_tokens=special_tokens,
        use_parallel=False,  # å…³é—­å¹¶è¡Œä»¥ä¸“æ³¨æµ‹è¯•åˆå¹¶ä¼˜åŒ–
        use_optimized_merge=True
    )
    opt_time = time.time() - start_time
    
    print(f"\nğŸ“Š ä¼˜åŒ–ç®—æ³•ç»“æœ:")
    print(f"   ç”¨æ—¶: {opt_time:.3f}ç§’")
    print(f"   æœ€ç»ˆè¯æ±‡è¡¨å¤§å°: {len(vocab_opt)}")
    print(f"   åˆå¹¶æ¬¡æ•°: {len(merges_opt)}")
    
    print("-" * 70)
    
    # æµ‹è¯•ä¼ ç»Ÿç®—æ³•
    print("ğŸŒ æµ‹è¯•ä¼ ç»Ÿçš„åˆå¹¶ç®—æ³•...")
    start_time = time.time()
    vocab_trad, merges_trad = train_bpe(
        input_path=test_file,
        vocab_size=vocab_size,
        special_tokens=special_tokens,
        use_parallel=False,  # å…³é—­å¹¶è¡Œä»¥ä¸“æ³¨æµ‹è¯•åˆå¹¶ä¼˜åŒ–
        use_optimized_merge=False
    )
    trad_time = time.time() - start_time
    
    print(f"\nğŸ“Š ä¼ ç»Ÿç®—æ³•ç»“æœ:")
    print(f"   ç”¨æ—¶: {trad_time:.3f}ç§’")
    print(f"   æœ€ç»ˆè¯æ±‡è¡¨å¤§å°: {len(vocab_trad)}")
    print(f"   åˆå¹¶æ¬¡æ•°: {len(merges_trad)}")
    
    print("\n" + "=" * 70)
    print("æ€§èƒ½å¯¹æ¯”æ€»ç»“")
    print("=" * 70)
    
    speedup = trad_time / opt_time if opt_time > 0 else 1
    time_saved = trad_time - opt_time
    
    print(f"ä¼˜åŒ–ç®—æ³•ç”¨æ—¶:   {opt_time:.3f}ç§’")
    print(f"ä¼ ç»Ÿç®—æ³•ç”¨æ—¶:   {trad_time:.3f}ç§’")
    print(f"æ—¶é—´èŠ‚çœ:       {time_saved:.3f}ç§’")
    print(f"æ€§èƒ½æå‡:       {speedup:.2f}x")
    
    if speedup > 1.1:
        print("âœ… ä¼˜åŒ–æ•ˆæœæ˜¾è‘—ï¼")
    elif speedup > 1.0:
        print("âœ… æœ‰ä¸€å®šä¼˜åŒ–æ•ˆæœ")
    else:
        print("âš ï¸  ä¼˜åŒ–æ•ˆæœä¸æ˜æ˜¾ï¼ˆå¯èƒ½æ•°æ®é‡å¤ªå°ï¼‰")
    
    # éªŒè¯ç»“æœä¸€è‡´æ€§
    if len(vocab_opt) == len(vocab_trad) and len(merges_opt) == len(merges_trad):
        print("âœ… ä¸¤ç§ç®—æ³•äº§ç”Ÿç›¸åŒçš„ç»“æœ")
    else:
        print("âš ï¸  ä¸¤ç§ç®—æ³•ç»“æœç•¥æœ‰ä¸åŒï¼ˆå¯èƒ½ç”±äºå®ç°ç»†èŠ‚å·®å¼‚ï¼‰")
    
    print("\nä¼˜åŒ–æŠ€æœ¯è¯´æ˜:")
    print("1. é…å¯¹é¢‘ç‡ç¼“å­˜: é¿å…æ¯æ¬¡é‡æ–°è®¡ç®—æ‰€æœ‰é…å¯¹é¢‘ç‡")
    print("2. å¢é‡æ›´æ–°: åªæ›´æ–°å—å½±å“çš„é…å¯¹è®¡æ•°")
    print("3. ç´¢å¼•ä¼˜åŒ–: å¿«é€ŸæŸ¥æ‰¾åŒ…å«ç‰¹å®šé…å¯¹çš„è¯æ±‡")
    
    # æ¸…ç†æµ‹è¯•æ–‡ä»¶
    if os.path.exists(test_file):
        os.remove(test_file)
        print(f"\næ¸…ç†æµ‹è¯•æ–‡ä»¶: {test_file}")

if __name__ == "__main__":
    test_merge_optimization()
